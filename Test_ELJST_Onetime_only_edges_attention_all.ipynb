{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import combinations\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bert_embedding import BertEmbedding\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import *\n",
    "import torch\n",
    "import keras\n",
    "\n",
    "import imp, gzip\n",
    "import pickle, nltk\n",
    "import gensim\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as my_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(i):\n",
    "    t = np.where(i>0)[0]\n",
    "    comb = combinations(t, 2)\n",
    "    embeds = {j:[] for j in t}\n",
    "\n",
    "    for p, q in comb:\n",
    "        if word_similarity[p][q]:\n",
    "            embeds[p] += [q]\n",
    "            embeds[q] += [p]\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"amazon_home_20000\"\n",
    "\n",
    "min_df = 5\n",
    "max_df = .5\n",
    "max_features = 50000\n",
    "cutoffs = [0.3, 0.6]\n",
    "\n",
    "n_cores = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"datasets/\" + dataset_name + \"_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,\n",
    "                             stop_words=\"english\", max_features=max_features,\n",
    "                             max_df=max_df, min_df=min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordOccurenceMatrix = vectorizer.fit_transform(dataset.text.tolist()).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "barren = np.where(wordOccurenceMatrix.sum(1)<=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10465, 15945])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embedding & Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_name = 'bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(pretrained_weights, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(vocab_file='resources/archive/bertvocab_' + dataset_name + '.txt', never_split=True, do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = [tokenizer.tokenize(i) for i in dataset.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:30<00:00, 656.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.9 s, sys: 134 ms, total: 25.1 s\n",
      "Wall time: 30.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = []\n",
    "for i in tqdm(tokenized_text):\n",
    "    t = [j for j in i if j in words]\n",
    "    temp.append(t)\n",
    "    \n",
    "tokenized_text = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = keras.preprocessing.sequence.pad_sequences(indexed_tokens, padding='post', dtype='long', maxlen=max([len(i) for i in indexed_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.split(input_ids, 1000, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pad_length = [len(i) for i in indexed_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "similar_words_bert = []\n",
    "similar_words_bert_attention = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "for batch in input_ids:\n",
    "\n",
    "    all_embeddings, _, _, all_attentions = model(batch)\n",
    "    for one_attentions in all_attentions[0].detach().numpy():\n",
    "\n",
    "#         one_side_edges = np.argmax(one_attentions[9], axis=1) #taking 9 layer of attention\n",
    "        embeds = {words.index(j):[] for j in tokenized_text[idx]}\n",
    "        \n",
    "        for one_side_edges in np.argmax(one_attentions, axis=2): #taking all layers top confident value\n",
    "            for j, i in enumerate(one_side_edges[:pad_length[idx]]):\n",
    "                if i < pad_length[idx]:\n",
    "                    embeds[words.index(tokenized_text[idx][i])] += [words.index(tokenized_text[idx][j])]\n",
    "            \n",
    "            for k, a in embeds.items():\n",
    "                b = list(set(a))\n",
    "                if k in b:\n",
    "                    b.remove(k)\n",
    "                embeds[k] = b\n",
    "            \n",
    "            similar_words_bert_attention.append(embeds)\n",
    "        idx += 1\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"resources/\"+ dataset_name +\"_\" + 'bert_attention_all'+ \".pickle\",\"wb\")\n",
    "pickle.dump(similar_words_bert_attention, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for batch in tqdm(input_ids):\n",
    "\n",
    "#     all_embeddings, _, _, all_attentions = model(batch)\n",
    "#     idx_copy = deepcopy(idx)\n",
    "    \n",
    "#     print(idx)\n",
    "#     for one_embedding in all_embeddings.detach().numpy():\n",
    "#         word_embeddings = one_embedding[:pad_length[idx]]\n",
    "#         word_similarity = cosine_similarity(word_embeddings)\n",
    "#         remove = np.where(word_similarity == 1.000) # to remove self words coupling\n",
    "\n",
    "#         for i, j in zip(remove[0], remove[1]):\n",
    "#             word_similarity[i][j] = 0\n",
    "#             word_similarity[j][i] = 0\n",
    "\n",
    "#         word_similarity = word_similarity > cutoff\n",
    "#         word_similarity = word_similarity.astype(int)\n",
    "#         np.fill_diagonal(word_similarity, 0)\n",
    "\n",
    "#         inds = np.where(word_similarity==1)\n",
    "#         embeds = {words.index(j):[] for j in tokenized_text[idx]}\n",
    "\n",
    "#         for i, j in zip(inds[0], inds[1]):\n",
    "#             embeds[words.index(tokenized_text[idx][i])] += [words.index(tokenized_text[idx][j])]\n",
    "#         similar_words_bert.append(embeds)\n",
    "    \n",
    "#     idx = deepcopy(idx_copy)\n",
    "    \n",
    "#     print(idx)\n",
    "    \n",
    "#     for one_attentions in all_attentions[0].detach().numpy():\n",
    "\n",
    "#         one_side_edges = np.argmax(one_attentions[9], axis=1) #taking 9 layer of attention\n",
    "#         embeds = {words.index(j):[] for j in tokenized_text[idx]}\n",
    "\n",
    "#         for j, i in enumerate(one_side_edges[:pad_length[idx]]):\n",
    "#             if i < pad_length[idx]:\n",
    "#                 embeds[words.index(tokenized_text[idx][i])] += [words.index(tokenized_text[idx][j])]\n",
    "#         similar_words_bert_attention.append(embeds)\n",
    "#         idx += 1\n",
    "#     print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) +\"_\" + 'bert' + \"_\" + str(cutoff) + \".pickle\",\"wb\")\n",
    "# pickle.dump(similar_words_bert, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) +\"_\" + 'bert_attention'+ \".pickle\",\"wb\")\n",
    "# pickle.dump(similar_words_bert_attention, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ### POS\n",
    "#         pp = np.array([i[1] for i in nltk.pos_tag(words)])\n",
    "#         pp[pp=='JJ'] = 1\n",
    "#         pp[pp=='JJR'] = 1\n",
    "#         pp[pp=='JJS'] = 1\n",
    "#         pp[pp=='NN'] = 1\n",
    "#         pp[pp=='NNS'] = 1\n",
    "#         pp[pp=='NNP'] = 1\n",
    "#         pp[pp=='NNPS'] = 1\n",
    "#         pp[pp!='1'] = 0\n",
    "#         pp = pp.astype(int)\n",
    "\n",
    "#         wordOccuranceMatrixBinary[:, np.where(pp!=1)[0]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordOccuranceMatrixBinary[0].sum()\n",
    "\n",
    "# np.sum(wordOccuranceMatrixBinary)\n",
    "\n",
    "# Counter(np.array([i[1] for i in nltk.pos_tag(words)]))\n",
    "\n",
    "# pp.sum()\n",
    "\n",
    "# np.where(pp!=1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for embedding_name in ['bert', 'elmo']:\n",
    "#     for cutoff in cutoffs:\n",
    "#         print(embedding_name, cutoff)\n",
    "#         pool = multiprocessing.Pool(n_cores)\n",
    "#         similar_words = pool.map(get_edges_transformers, dataset.text.tolist())\n",
    "#         pool.close()\n",
    "#         pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) + \"_\" + embedding_name + \"_\" + str(cutoff) + \".pickle\",\"wb\")\n",
    "#         pickle.dump(similar_words, pickle_out)\n",
    "#         pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     pd = pd.apply(lambda x: convert_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_df(df):\n",
    "#     df['text'] = preprocess(df['reviewText'])\n",
    "    \n",
    "# #     pool = multiprocessing.Pool(n_cores)\n",
    "# #     df['cleaned'] = pool.map(process_l, df['text'].tolist())\n",
    "# #     pool.close()\n",
    "    \n",
    "# #     df['text'] = df['cleaned'].apply(lambda x: \" \".join(x))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = [item for sublist in dataset['cleaned'].tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(Counter(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_l(s):\n",
    "#     return [i.lemma_ for i in sp(s) if i.lemma_ not in '-PRON-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = dataset['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = multiprocessing.Pool(n_cores)\n",
    "# processed_l = pool.map(process_l, l)\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(sampler, \"resources/sampler_20iter_0.5_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/amazon_muiscal_glove_0.4.pickle\",\"wb\")\n",
    "# pickle.dump(similar_words, pickle_out)\n",
    "# pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_new",
   "language": "python",
   "name": "python3_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
